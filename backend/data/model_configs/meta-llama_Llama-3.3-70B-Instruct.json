{
  "model_id": "meta-llama_Llama-3.3-70B-Instruct",
  "hf_model_id": "meta-llama/Llama-3.3-70B-Instruct",
  "name": "Llama-3.3-70B-Instruct",
  "hidden_size": 8192,
  "num_layers": 80,
  "vocab_size": 128256,
  "total_params": 69502369792,
  "total_params_formatted": "69.5B",
  "layer_types": [
    {
      "name": "attention",
      "class": "GroupedQueryAttentionLayer",
      "count": 80,
      
      "specs": {
        "layer_idx": 0,
        "input_dim": 8192,
        "output_dim": 8192,
        "parameter_count": 150994944,
        "hidden_size": 8192,
        "num_heads": 64,
        "num_kv_heads": 8,
        "head_dim": 128
      }
    },
    {
      "name": "feedforward",
      "class": "GatedMLPLayer",
      "count": 80,
      "specs": {
        "layer_idx": 0,
        "input_dim": 8192,
        "output_dim": 8192,
        "parameter_count": 704643072,
        "hidden_size": 8192,
        "intermediate_size": 28672
      }
    },
    {
      "name": "norm",
      "class": "NormLayer",
      "count": 80,
      "specs": {
        "layer_idx": 0,
        "input_dim": 8192,
        "output_dim": 8192,
        "parameter_count": 8192,
        "hidden_size": 8192,
        "has_bias": false
      }
    },
    {
      "name": "embedding",
      "class": "EmbeddingLayer",
      "count": 1,
      "specs": {
        "vocab_size": 128256,
        "hidden_size": 8192,
        "parameter_count": 1050673152
      }
    }
  ],
  "validated": true,
  "validation_notes": "Llama 3.3 70B confirmed (69.5B params), GQA (8 KV heads), SwiGLU MLP (28672 intermediate), 80 layers, embedding included"
}