{
  "model_id": "TinyLlama_TinyLlama-1.1B-Chat-v1.0",
  "hf_model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "name": "TinyLlama-1.1B-Chat-v1.0",
  "hidden_size": 2048,
  "num_layers": 22,
  "vocab_size": 32000,
  "total_params": 1034465280,
  "total_params_formatted": "1.0B",
  "layer_types": [
    {
      "name": "attention",
      "class": "GroupedQueryAttentionLayer",
      "count": 22,
      "specs": {
        "layer_idx": 0,
        "input_dim": 2048,
        "output_dim": 2048,
        "parameter_count": 9437184,
        "hidden_size": 2048,
        "num_heads": 32,
        "num_kv_heads": 4,
        "head_dim": 64
      }
    },
    {
      "name": "feedforward",
      "class": "GatedMLPLayer",
      "count": 22,
      "specs": {
        "layer_idx": 0,
        "input_dim": 2048,
        "output_dim": 2048,
        "parameter_count": 34603008,
        "hidden_size": 2048,
        "intermediate_size": 5632
      }
    },
    {
      "name": "norm",
      "class": "NormLayer",
      "count": 22,
      "specs": {
        "layer_idx": 0,
        "input_dim": 2048,
        "output_dim": 2048,
        "parameter_count": 2048,
        "hidden_size": 2048,
        "has_bias": false
      }
    },
    {
      "name": "embedding",
      "class": "EmbeddingLayer",
      "count": 1,
      "specs": {
        "vocab_size": 32000,
        "hidden_size": 2048,
        "parameter_count": 65536000
      }
    }
  ],
  "validated": true,
  "validation_notes": "1.0B params confirmed, GQA (4 KV heads), SwiGLU MLP, 22 layers, embedding included"
}